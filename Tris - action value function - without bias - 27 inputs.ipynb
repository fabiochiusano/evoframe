{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.reward_builders import RewardBuilderGame\n",
    "from evoframe.population_update_builders import PopulationUpdateBuilderStatic\n",
    "from evoframe.selector_function import SelectorFunctionFactory\n",
    "from evoframe import EvolutionBuilder\n",
    "from evoframe.models import FeedForwardNetwork\n",
    "from evoframe.models import ActivationFunctions\n",
    "from evoframe.games import Game\n",
    "from evoframe import get_agent_wrapper_func\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from evoframe.experiment_results import *\n",
    "from evoframe.utility import clean_experiment_directory\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.reward_builders.reward_builder_game import TournamentMode\n",
    "from evoframe.games import Tris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game\n",
    "game_creation_func = lambda context: Tris()\n",
    "\n",
    "# Model\n",
    "layer_sizes = [27, 18, 12, 9]\n",
    "get_model_func = lambda: FeedForwardNetwork(layer_sizes, ActivationFunctions.get_sigmoid(), ActivationFunctions.get_sigmoid(), with_bias=False)\n",
    "\n",
    "# Game-Model interface\n",
    "def predict_func(model, game):\n",
    "    # In Tris, 'inputs' is a 3x3 np.array, thus we must flatten it to a 9x1 np.array\n",
    "    # output is a 9x1 vector, where the cell with highest value corresponding\n",
    "    # to a valid move will be the chosen move\n",
    "    flattened_board = game.board.flatten()\n",
    "    flattened_board_expanded = [[1,0,0] if cell == 1 else [0,1,0] if cell == 0 else [0,0,1] for cell in flattened_board]\n",
    "    flattened_board_expanded = np.array([cell for expanded_cell in flattened_board_expanded for cell in expanded_cell])\n",
    "    return model.predict(flattened_board_expanded)[0]\n",
    "agent_wrapper_func = get_agent_wrapper_func(predict_func)\n",
    "\n",
    "# Reward function and update env function\n",
    "# Since Tris is a two-players game, we can compute the reward with a tournament against the current generation\n",
    "keep_only = 40\n",
    "reward_func, get_context_func = RewardBuilderGame() \\\n",
    ".with_game_creation_func(game_creation_func) \\\n",
    ".with_agent_wrapper_func(agent_wrapper_func) \\\n",
    ".with_competitive_tournament(TournamentMode.VS_BESTS_RANDOM) \\\n",
    ".with_keep_only(keep_only) \\\n",
    ".with_weight_normalization(2,2) \\\n",
    ".get()\n",
    "\n",
    "\n",
    "# Update population function\n",
    "get_new_pop_func = PopulationUpdateBuilderStatic() \\\n",
    ".add_operator(\"es_1_copy\", 0.1) \\\n",
    ".add_operator(\"es_1_mutation\", 0.1, 0.1, 0.1) \\\n",
    ".add_operator(\"es_1_mutation\", 0.1, 0.1, 0.5) \\\n",
    ".add_operator(\"es_1_mutation\", 0.1, 0.3, 0.3) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.01, 0.1, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.01, 0.2, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.01, 0.3, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.05, 0.1, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.05, 0.2, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.05, 0.3, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.1, 0.1, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.1, 0.2, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient_and_mutation\", 0.1, 0.1, 0.3, 1) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.01) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.05) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1) \\\n",
    ".add_selector_func(SelectorFunctionFactory.get_geometric_selector_function(0.3)) \\\n",
    ".get()\n",
    "\n",
    "# Evolution function\n",
    "evolution_func = EvolutionBuilder() \\\n",
    "    .with_get_model_func(get_model_func) \\\n",
    "    .with_reward_func(reward_func) \\\n",
    "    .with_get_new_pop_func(get_new_pop_func) \\\n",
    "    .with_get_context_func(get_context_func) \\\n",
    "    .get()\n",
    "\n",
    "pop_size = 100\n",
    "num_epochs = 10\n",
    "experiment_name = \"tris\"\n",
    "clean_experiment_directory(experiment_name)\n",
    "evolution_func(experiment_name, pop_size, num_epochs, num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 100\n",
    "num_epochs = 10\n",
    "experiment_name = \"tris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_input_func():\n",
    "    board = np.array([[np.random.choice([-1,0,1]) for i in range(3)] for j in range(3)])\n",
    "    board_flattened = board.flatten()\n",
    "    board_repr = [[1,0,0] if cell == 1 else [0,1,0] if cell == 0 else [0,0,1] for cell in board_flattened]\n",
    "    board_repr_expanded = np.array([cell for expanded_cell in board_repr for cell in expanded_cell])\n",
    "    return board_repr_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_behavioural_variances_to_input(experiment_name, get_random_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def show_predictions(model, inp):\n",
    "    game = Tris()\n",
    "    game.board = inp\n",
    "    out = np.array(model.predict(game))\n",
    "    out = out.reshape((3,3))\n",
    "    shape = out.shape\n",
    "    data = [(row+1, col+1, out[row][col]) for row in range(shape[0]) for col in range(shape[1])]\n",
    "    columns = [\"neuron_input\", \"neuron_output\", \"value\"]\n",
    "    df = pd.DataFrame(data=data, columns=columns)\n",
    "    return px.density_heatmap(df, x=\"neuron_output\", y=\"neuron_input\", z=\"value\",\n",
    "                                         histfunc=\"sum\", color_continuous_scale=\"RdYlGn\", range_color=(-2,2),\n",
    "                                         nbinsx=shape[1], nbinsy=shape[0],\n",
    "                                         range_x=(0.5, shape[1]+0.5), range_y=(0.5, shape[0]+0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game-Model interface\n",
    "def predict_func(model, game):\n",
    "    # In Tris, 'inputs' is a 3x3 np.array, thus we must flatten it to a 9x1 np.array\n",
    "    # output is a 9x1 vector, where the cell with highest value corresponding\n",
    "    # to a valid move will be the chosen move\n",
    "    flattened_board = game.board.flatten()\n",
    "    flattened_board_expanded = [[1,0,0] if cell == 1 else [0,1,0] if cell == 0 else [0,0,1] for cell in flattened_board]\n",
    "    flattened_board_expanded = np.array([cell for expanded_cell in flattened_board_expanded for cell in expanded_cell])\n",
    "    return model.predict(flattened_board_expanded)[0]\n",
    "agent_wrapper_func = get_agent_wrapper_func(predict_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent_wrapper_func(pickle_load_best_model_of_epoch(experiment_name, num_epochs, pop_size))\n",
    "inp = get_random_input_func()\n",
    "inp_repr = []\n",
    "print(inp)\n",
    "for i in range(9):\n",
    "    sublist = inp[i*3:(i*3)+3]\n",
    "    inp_repr.append(1 if sublist.tolist() == [1,0,0] else 0 if sublist.tolist() == [0,1,0] else -1)\n",
    "inp_repr = [inp_repr[x:x+3] for x in range(0, len(inp_repr), 3)]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print(inp_repr[2-i][j], end=\" \")\n",
    "    print(\"\")\n",
    "show_predictions(model, np.array(inp_repr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(show_best_fnn_weights,\n",
    "         experiment_name=fixed(experiment_name),\n",
    "         epoch=widgets.IntSlider(min=1, max=num_epochs, step=1, value=1),\n",
    "         with_bias=fixed(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_behavioural_differences(experiment_name, get_random_input_func, mode=\"first_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = plot_params_statistics(experiment_name, with_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tournament(experiment_name, agent_wrapper_func, Tris, 5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_operators_statistics_means(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_operators_statistics_max(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentHuman:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, game):\n",
    "        move = int(input(\"Select move: \"))\n",
    "        prediction = [1 if i == move else 0 for i in range(9)]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Tris()\n",
    "game.play(AgentHuman(), agent_wrapper_func(pickle_load_best_model_of_epoch(experiment_name, num_epochs, pop_size)), interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
