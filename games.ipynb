{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.reward_builders import RewardBuilderGame\n",
    "from evoframe.population_update_builders import PopulationUpdateBuilderStatic\n",
    "from evoframe.selector_function import SelectorFunctionFactory\n",
    "from evoframe import PopulationManager\n",
    "from evoframe.models import FeedForwardNetwork\n",
    "from evoframe.models import ActivationFunctions\n",
    "from evoframe.games import Game\n",
    "from evoframe import get_agent_wrapper_func\n",
    "import evoframe.func_with_context as fwc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from evoframe.experiment_results import plot_rewards, show_best_fnn_weights, get_best_model_of_epoch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def recursively_default_dict():\n",
    "    return collections.defaultdict(recursively_default_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def to_gif(experiment_name, num_epochs, duration=0.5):\n",
    "    for i in range(1, num_epochs+1):\n",
    "        print(i)\n",
    "        f = show_best_fnn_weights(experiment_name, i)\n",
    "        b = f.to_image(format=\"png\")\n",
    "        with open(\"images/image{}.png\".format(i), \"wb\") as fi:\n",
    "            fi.write(b)\n",
    "    images = []\n",
    "    filenames = [\"images/image{}.png\".format(i) for i in range(1, num_epochs+1)]\n",
    "    for i,filename in enumerate(filenames):\n",
    "        print(i)\n",
    "        images.append(imageio.imread(filename))\n",
    "    imageio.mimsave('nn.gif', images, duration=duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.games import GuessPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define global context\n",
    "context = recursively_default_dict()\n",
    "\n",
    "# Game\n",
    "game_creation_func = lambda context: GuessPoint(np.array([0.2,0.8,0.5]), np.array([0.4, 0.5, 10]))\n",
    "game_creation_func = fwc.func_with_context(game_creation_func, context=context)\n",
    "\n",
    "# Model\n",
    "layer_sizes = [3, 5, 3]\n",
    "get_model_func = lambda context: FeedForwardNetwork(layer_sizes, ActivationFunctions.get_relu(), ActivationFunctions.get_id())\n",
    "get_model_func = fwc.func_with_context(get_model_func, context=context)\n",
    "\n",
    "# Game-Model interface\n",
    "predict_func = lambda model, inputs: model.predict(inputs)\n",
    "agent_wrapper_func = get_agent_wrapper_func(predict_func)\n",
    "\n",
    "# Reward function and update env function\n",
    "reward_function = RewardBuilderGame() \\\n",
    ".with_game_creation_function(game_creation_func) \\\n",
    ".with_agent_wrapper_func(agent_wrapper_func) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Update population function\n",
    "get_new_pop_f = PopulationUpdateBuilderStatic() \\\n",
    ".add_operator(\"es_1_mutation\", 1, 0.3, 0.3) \\\n",
    ".add_operator(\"es_2_crossover\", 0.01, 0.8) \\\n",
    ".add_operator(\"es_1_copy\", 0.01) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.01) \\\n",
    ".add_selector_f(SelectorFunctionFactory.get_geometric_selector_function(0.3)) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Population manager\n",
    "pop_size = 500\n",
    "pm = PopulationManager(pop_size, get_model_func, reward_function, get_new_pop_f, context)\n",
    "\n",
    "# Run population manager\n",
    "num_epochs = 10\n",
    "experiment_name = \"guesspoint\"\n",
    "pm.run(num_epochs, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gif(experiment_name, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![...](nn.gif \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(show_best_fnn_weights,\n",
    "         experiment_name=fixed(experiment_name),\n",
    "         epoch=widgets.IntSlider(min=1, max=num_epochs, step=1, value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.games import GuessFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define global context\n",
    "context = recursively_default_dict()\n",
    "\n",
    "# Game\n",
    "game_func = lambda i: np.array([2*i[0]-3*i[1]+4, i[1]-8*i[2]-5])\n",
    "input_dim = 3\n",
    "input_domains = [(-1,1),(-1,1),(3,7)]\n",
    "sample_every = [0.1, 0.1, 0.3]\n",
    "game_creation_function = lambda context: GuessFunction(game_func, input_dim, input_domains, sample_every)\n",
    "game_creation_function = fwc.func_with_context(game_creation_function, context=context)\n",
    "\n",
    "# Model\n",
    "layer_sizes = [3, 5, 2]\n",
    "get_model_func = lambda context: FeedForwardNetwork(layer_sizes, ActivationFunctions.get_arctan(), ActivationFunctions.get_id())\n",
    "get_model_func = fwc.func_with_context(get_model_func, context=context)\n",
    "\n",
    "# Game-Model interface\n",
    "predict_func = lambda model, inputs: model.predict(inputs)\n",
    "agent_wrapper_func = get_agent_wrapper_func(predict_func)\n",
    "\n",
    "# Reward function and update env function\n",
    "reward_function = RewardBuilderGame() \\\n",
    ".with_game_creation_function(game_creation_function) \\\n",
    ".with_agent_wrapper_func(agent_wrapper_func) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Update population function\n",
    "get_new_pop_f = PopulationUpdateBuilderStatic() \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.1, 0.1) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.1, 0.5) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.3, 0.3) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.5, 0.5) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.5, 0.1) \\\n",
    ".add_operator(\"es_2_crossover\", 0.1, 0.8) \\\n",
    ".add_operator(\"es_1_copy\", 0.1) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1) \\\n",
    ".add_selector_f(SelectorFunctionFactory.get_geometric_selector_function(0.3)) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Population manager\n",
    "pop_size = 50\n",
    "pickle_models_after_gens = 1\n",
    "pm = PopulationManager(pop_size, get_model_func, reward_function, get_new_pop_f, pickle_models_after_gens, context)\n",
    "\n",
    "# Run population manager\n",
    "num_epochs = 50\n",
    "experiment_name = \"guessfunction\"\n",
    "pm.run(num_epochs, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_rewards(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_gif(experiment_name, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.experiment_results import get_best_model_of_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([0.5,0.5,5])\n",
    "get_best_model_of_epoch(experiment_name, num_epochs).predict(test_array), game_func(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interact(show_best_fnn_weights,\n",
    "         experiment_name=fixed(experiment_name),\n",
    "         epoch=widgets.IntSlider(min=1, max=num_epochs, step=1, value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Tris, 9 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.reward_builders.reward_builder_game import TournamentMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.games import Tris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, best reward is 2\n",
      "Number of peaks is 0\n",
      "Epoch 2, best reward is 4\n",
      "Number of peaks is 0\n",
      "Epoch 3, best reward is 5\n",
      "Number of peaks is 0\n",
      "Epoch 4, best reward is 6\n",
      "Number of peaks is 0\n",
      "Epoch 5, best reward is 6\n",
      "Number of peaks is 0\n",
      "Epoch 6, best reward is 8\n",
      "Number of peaks is 1\n",
      "Epoch 7, best reward is 10\n",
      "Number of peaks is 1\n",
      "Epoch 8, best reward is 13\n",
      "Number of peaks is 1\n",
      "Epoch 9, best reward is 13\n",
      "Number of peaks is 1\n",
      "Epoch 10, best reward is 14\n",
      "Number of peaks is 2\n",
      "Epoch 11, best reward is 16\n",
      "Number of peaks is 2\n",
      "Epoch 12, best reward is 16\n",
      "Number of peaks is 2\n",
      "Epoch 13, best reward is 22\n",
      "Number of peaks is 3\n",
      "Epoch 14, best reward is 22\n",
      "Number of peaks is 3\n",
      "Epoch 15, best reward is 22\n",
      "Number of peaks is 4\n",
      "Epoch 16, best reward is 22\n",
      "Number of peaks is 4\n",
      "Epoch 17, best reward is 22\n",
      "Number of peaks is 4\n",
      "Epoch 18, best reward is 32\n",
      "Number of peaks is 4\n",
      "Epoch 19, best reward is 32\n",
      "Number of peaks is 4\n",
      "Epoch 20, best reward is 30\n",
      "Number of peaks is 5\n",
      "Epoch 21, best reward is 30\n",
      "Number of peaks is 5\n",
      "Epoch 22, best reward is 32\n",
      "Number of peaks is 5\n",
      "Epoch 23, best reward is 33\n",
      "Number of peaks is 5\n",
      "Epoch 24, best reward is 39\n",
      "Number of peaks is 5\n",
      "Epoch 25, best reward is 40\n",
      "Number of peaks is 5\n",
      "Epoch 26, best reward is 40\n",
      "Number of peaks is 5\n",
      "Epoch 27, best reward is 38\n",
      "Number of peaks is 6\n",
      "Epoch 28, best reward is 38\n",
      "Number of peaks is 6\n",
      "Epoch 29, best reward is 36\n",
      "Number of peaks is 6\n",
      "Epoch 30, best reward is 35\n",
      "Number of peaks is 6\n",
      "Epoch 31, best reward is 33\n",
      "Number of peaks is 6\n",
      "Epoch 32, best reward is 31\n",
      "Number of peaks is 6\n",
      "Epoch 33, best reward is 29\n",
      "Number of peaks is 6\n",
      "Epoch 34, best reward is 27\n",
      "Number of peaks is 6\n",
      "Epoch 35, best reward is 25\n",
      "Number of peaks is 6\n",
      "Epoch 36, best reward is 31\n",
      "Number of peaks is 6\n",
      "Epoch 37, best reward is 32\n",
      "Number of peaks is 6\n",
      "Epoch 38, best reward is 33\n",
      "Number of peaks is 6\n",
      "Epoch 39, best reward is 34\n",
      "Number of peaks is 6\n",
      "Epoch 40, best reward is 35\n",
      "Number of peaks is 6\n",
      "Epoch 41, best reward is 36\n",
      "Number of peaks is 6\n",
      "Epoch 42, best reward is 40\n",
      "Number of peaks is 6\n",
      "Epoch 43, best reward is 38\n",
      "Number of peaks is 6\n",
      "Epoch 44, best reward is 36\n",
      "Number of peaks is 7\n",
      "Epoch 45, best reward is 34\n",
      "Number of peaks is 7\n",
      "Epoch 46, best reward is 36\n",
      "Number of peaks is 7\n",
      "Epoch 47, best reward is 36\n",
      "Number of peaks is 7\n",
      "Epoch 48, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 49, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 50, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 51, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 52, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 53, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 54, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 55, best reward is 34\n",
      "Number of peaks is 8\n",
      "Epoch 56, best reward is 32\n",
      "Number of peaks is 8\n",
      "Epoch 57, best reward is 30\n",
      "Number of peaks is 8\n",
      "Epoch 58, best reward is 28\n",
      "Number of peaks is 8\n",
      "Epoch 59, best reward is 26\n",
      "Number of peaks is 8\n",
      "Epoch 60, best reward is 24\n",
      "Number of peaks is 8\n",
      "Epoch 61, best reward is 25\n",
      "Number of peaks is 8\n",
      "Epoch 62, best reward is 34\n",
      "Number of peaks is 8\n",
      "Epoch 63, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 64, best reward is 36\n",
      "Number of peaks is 8\n",
      "Epoch 65, best reward is 34\n",
      "Number of peaks is 9\n",
      "Epoch 66, best reward is 34\n",
      "Number of peaks is 9\n",
      "Epoch 67, best reward is 36\n",
      "Number of peaks is 9\n",
      "Epoch 68, best reward is 40\n",
      "Number of peaks is 9\n",
      "Epoch 69, best reward is 40\n",
      "Number of peaks is 9\n",
      "Epoch 70, best reward is 38\n",
      "Number of peaks is 10\n",
      "Epoch 71, best reward is 38\n",
      "Number of peaks is 10\n",
      "Epoch 72, best reward is 36\n",
      "Number of peaks is 10\n",
      "Epoch 73, best reward is 34\n",
      "Number of peaks is 10\n",
      "Epoch 74, best reward is 34\n",
      "Number of peaks is 10\n",
      "Epoch 75, best reward is 34\n",
      "Number of peaks is 10\n",
      "Epoch 76, best reward is 32\n",
      "Number of peaks is 10\n",
      "Epoch 77, best reward is 36\n",
      "Number of peaks is 10\n",
      "Epoch 78, best reward is 36\n",
      "Number of peaks is 10\n",
      "Epoch 79, best reward is 34\n",
      "Number of peaks is 11\n",
      "Epoch 80, best reward is 34\n",
      "Number of peaks is 11\n",
      "Epoch 81, best reward is 34\n",
      "Number of peaks is 11\n",
      "Epoch 82, best reward is 36\n",
      "Number of peaks is 11\n",
      "Epoch 83, best reward is 36\n",
      "Number of peaks is 11\n",
      "Epoch 84, best reward is 34\n",
      "Number of peaks is 12\n",
      "Epoch 85, best reward is 34\n",
      "Number of peaks is 12\n",
      "Epoch 86, best reward is 34\n",
      "Number of peaks is 12\n",
      "Epoch 87, best reward is 34\n",
      "Number of peaks is 12\n",
      "Epoch 88, best reward is 34\n",
      "Number of peaks is 12\n",
      "Epoch 89, best reward is 34\n",
      "Number of peaks is 12\n",
      "Epoch 90, best reward is 32\n",
      "Number of peaks is 12\n",
      "Epoch 91, best reward is 30\n",
      "Number of peaks is 12\n",
      "Epoch 92, best reward is 28\n",
      "Number of peaks is 12\n",
      "Epoch 93, best reward is 26\n",
      "Number of peaks is 12\n",
      "Epoch 94, best reward is 29\n",
      "Number of peaks is 12\n",
      "Epoch 95, best reward is 30\n",
      "Number of peaks is 12\n",
      "Epoch 96, best reward is 29\n",
      "Number of peaks is 12\n",
      "Epoch 97, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 98, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 99, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 100, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 101, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 102, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 103, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 104, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 105, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 106, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 107, best reward is 27\n",
      "Number of peaks is 13\n",
      "Epoch 108, best reward is 26\n",
      "Number of peaks is 13\n",
      "Epoch 109, best reward is 25\n",
      "Number of peaks is 13\n",
      "Epoch 110, best reward is 24\n",
      "Number of peaks is 13\n",
      "Epoch 111, best reward is 23\n",
      "Number of peaks is 13\n",
      "Epoch 112, best reward is 22\n",
      "Number of peaks is 13\n",
      "Epoch 113, best reward is 21\n",
      "Number of peaks is 13\n",
      "Epoch 114, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 115, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 116, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 117, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 118, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 119, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 120, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 121, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 122, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 123, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 124, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 125, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 126, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 127, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 128, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 129, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 130, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 131, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 132, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 133, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 134, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 135, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 136, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 137, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 138, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 139, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 140, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 141, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 142, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 143, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 144, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 145, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 146, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 147, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 148, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 149, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 150, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 151, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 152, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 153, best reward is 20\n",
      "Number of peaks is 13\n",
      "Epoch 154, best reward is 20\n",
      "Number of peaks is 13\n"
     ]
    }
   ],
   "source": [
    "##### Define global context\n",
    "context = recursively_default_dict()\n",
    "\n",
    "# Game\n",
    "game_creation_function = lambda context: Tris()\n",
    "game_creation_function = fwc.func_with_context(game_creation_function, context=context)\n",
    "\n",
    "# Model\n",
    "layer_sizes = [9, 18, 9]\n",
    "get_model_func = lambda context: FeedForwardNetwork(layer_sizes, ActivationFunctions.get_sigmoid(), ActivationFunctions.get_sigmoid())\n",
    "get_model_func = fwc.func_with_context(get_model_func, context=context)\n",
    "\n",
    "# Game-Model interface\n",
    "def predict_func(model, inputs):\n",
    "    # In Tris, 'inputs' is a 3x3 np.array, thus we must flatten it to a 9x1 np.array\n",
    "    # output is a 9x1 vector, where the cell with highest value corresponding\n",
    "    # to a valid move will be the chosen move\n",
    "    return model.predict(inputs.flatten())[0]\n",
    "agent_wrapper_func = get_agent_wrapper_func(predict_func)\n",
    "\n",
    "# Reward function and update env function\n",
    "# Since Tris is a two-players game, we can compute the reward with a tournament against the current generation\n",
    "keep_only = 20\n",
    "reward_function = RewardBuilderGame() \\\n",
    ".with_game_creation_function(game_creation_function) \\\n",
    ".with_agent_wrapper_func(agent_wrapper_func) \\\n",
    ".with_competitive_tournament(TournamentMode.VS_PEAKS) \\\n",
    ".with_keep_only(keep_only) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Update population function\n",
    "get_new_pop_f = PopulationUpdateBuilderStatic() \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.1, 0.1) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.1, 0.5) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.3, 0.3) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.5, 0.5) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.5, 0.1) \\\n",
    ".add_operator(\"es_2_crossover\", 0.1, 0.8) \\\n",
    ".add_operator(\"es_1_copy\", 0.1) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.05) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.1) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.3) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.5) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 1.0) \\\n",
    ".add_selector_f(SelectorFunctionFactory.get_geometric_selector_function(0.3)) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Population manager\n",
    "pop_size = 50\n",
    "pm = PopulationManager(pop_size, get_model_func, reward_function, get_new_pop_f, context)\n",
    "\n",
    "# Run population manager\n",
    "num_epochs = 300\n",
    "experiment_name = \"tris\"\n",
    "pm.run(num_epochs, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_rewards(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.experiment_results import load_context\n",
    "\n",
    "c = load_context(experiment_name, epochs=range(1,num_epochs+1), keys=[\"rewards\"])\n",
    "best_r = []\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    rewards = c[\"epochs\"][epoch][\"rewards\"]\n",
    "    best_r.append(rewards[np.array(rewards).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_peaks = 0\n",
    "for r1,r2,r3 in zip(best_r, best_r[1:], best_r[2:]):\n",
    "    if r1 < r2 and r2 >= r3:\n",
    "        n_peaks += 1\n",
    "n_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_gif(experiment_name, num_epochs, duration=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(show_best_fnn_weights,\n",
    "         experiment_name=fixed(experiment_name),\n",
    "         epoch=widgets.IntSlider(min=1, max=num_epochs, step=1, value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentHuman:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, board):\n",
    "        move = int(input(\"Select move: \"))\n",
    "        prediction = [1 if i == move else 0 for i in range(9)]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game = Tris()\n",
    "game.play(AgentHuman(), agent_wrapper_func(get_best_model_of_epoch(experiment_name, num_epochs)), interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Tris, 18 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define global context\n",
    "context = recursively_default_dict()\n",
    "\n",
    "# Game\n",
    "game_creation_function = lambda context: Tris()\n",
    "game_creation_function = fwc.func_with_context(game_creation_function, context=context)\n",
    "\n",
    "# Model\n",
    "layer_sizes = [18, 9, 9]\n",
    "get_model_func = lambda context: FeedForwardNetwork(layer_sizes, ActivationFunctions.get_sigmoid(), ActivationFunctions.get_sigmoid())\n",
    "get_model_func = fwc.func_with_context(get_model_func, context=context)\n",
    "\n",
    "# Game-Model interface\n",
    "def predict_func(model, inputs):\n",
    "    # In Tris, 'inputs' is a 3x3 np.array\n",
    "    # output is a 9x1 vector, where the cell with highest value corresponding\n",
    "    # to a valid move will be the chosen move\n",
    "    board = inputs\n",
    "    inputs = []\n",
    "    for row in board:\n",
    "        for cell in row:\n",
    "            if cell == Tris.PLAYER_1:\n",
    "                inputs += [1, 0]\n",
    "            elif cell == Tris.PLAYER_2:\n",
    "                inputs += [0, 1]\n",
    "            else:\n",
    "                inputs += [0, 0]\n",
    "    return model.predict(np.array(inputs))[0]\n",
    "agent_wrapper_func = get_agent_wrapper_func(predict_func)\n",
    "\n",
    "# Reward function and update env function\n",
    "# Since Tris is a two-players game, we can compute the reward with a tournament against the current generation\n",
    "keep_only = 30\n",
    "reward_function = RewardBuilderGame() \\\n",
    ".with_game_creation_function(game_creation_function) \\\n",
    ".with_agent_wrapper_func(agent_wrapper_func) \\\n",
    ".with_competitive_tournament(TournamentMode.VS_BEST_OF_EACH_GEN) \\\n",
    ".with_keep_only(keep_only) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Update population function\n",
    "get_new_pop_f = PopulationUpdateBuilderStatic() \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.1, 0.1) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.1, 0.5) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.3, 0.3) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.5, 0.5) \\\n",
    ".add_operator(\"es_1_mutation\", 0.2, 0.5, 0.1) \\\n",
    ".add_operator(\"es_2_crossover\", 0.1, 0.8) \\\n",
    ".add_operator(\"es_1_copy\", 0.1) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1) \\\n",
    ".add_selector_f(SelectorFunctionFactory.get_geometric_selector_function(0.3)) \\\n",
    ".with_context(context) \\\n",
    ".get()\n",
    "\n",
    "# Population manager\n",
    "pop_size = 30\n",
    "pm = PopulationManager(pop_size, get_model_func, reward_function, get_new_pop_f, keep_only, context)\n",
    "\n",
    "# Run population manager\n",
    "num_epochs = 300\n",
    "experiment_name = \"tris_18\"\n",
    "pm.run(num_epochs, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(show_best_fnn_weights,\n",
    "         experiment_name=fixed(experiment_name),\n",
    "         epoch=widgets.IntSlider(min=1, max=num_epochs, step=1, value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gif(experiment_name, num_epochs, duration=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentHuman:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, board):\n",
    "        move = int(input(\"Select move: \"))\n",
    "        prediction = [1 if i == move else 0 for i in range(9)]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game = Tris()\n",
    "game.play(agent_wrapper_func(get_best_model_of_epoch(experiment_name, num_epochs)), AgentHuman(), interactive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
