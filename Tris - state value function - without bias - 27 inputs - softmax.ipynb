{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.reward_builders import RewardBuilderGame\n",
    "from evoframe.population_update_builders import PopulationUpdateBuilderStatic\n",
    "from evoframe.selector_function import SelectorFunctionFactory\n",
    "from evoframe import EvolutionBuilder\n",
    "from evoframe.models import FeedForwardNetwork\n",
    "from evoframe.models import ActivationFunctions\n",
    "from evoframe.games import Game\n",
    "from evoframe import get_agent_wrapper_func\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from evoframe.experiment_results import *\n",
    "from evoframe.utility import clean_experiment_directory\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evoframe.reward_builders.reward_builder_game import TournamentMode\n",
    "from evoframe.games import Tris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, best reward is 2\n",
      "Epoch 2, best reward is 4\n",
      "Epoch 3, best reward is 6\n",
      "Epoch 4, best reward is 4\n",
      "Epoch 5, best reward is 8\n",
      "Epoch 6, best reward is 5\n",
      "Epoch 7, best reward is 8\n",
      "Epoch 8, best reward is 7\n",
      "Epoch 9, best reward is 6\n",
      "Epoch 10, best reward is 8\n",
      "Epoch 11, best reward is 7\n",
      "Epoch 12, best reward is 10\n",
      "Epoch 13, best reward is 10\n",
      "Epoch 14, best reward is 8\n",
      "Epoch 15, best reward is 7\n",
      "Epoch 16, best reward is 7\n",
      "Epoch 17, best reward is 6\n",
      "Epoch 18, best reward is 8\n",
      "Epoch 19, best reward is 10\n",
      "Epoch 20, best reward is 10\n",
      "Epoch 21, best reward is 9\n",
      "Epoch 22, best reward is 11\n",
      "Epoch 23, best reward is 8\n",
      "Epoch 24, best reward is 9\n",
      "Epoch 25, best reward is 12\n",
      "Epoch 26, best reward is 8\n",
      "Epoch 27, best reward is 9\n",
      "Epoch 28, best reward is 11\n",
      "Epoch 29, best reward is 10\n",
      "Epoch 30, best reward is 13\n",
      "Epoch 31, best reward is 13\n",
      "Epoch 32, best reward is 9\n",
      "Epoch 33, best reward is 10\n",
      "Epoch 34, best reward is 11\n",
      "Epoch 35, best reward is 10\n",
      "Epoch 36, best reward is 10\n",
      "Epoch 37, best reward is 12\n",
      "Epoch 38, best reward is 11\n",
      "Epoch 39, best reward is 11\n",
      "Epoch 40, best reward is 11\n",
      "Epoch 41, best reward is 10\n",
      "Epoch 42, best reward is 9\n",
      "Epoch 43, best reward is 20\n",
      "Epoch 44, best reward is 16\n",
      "Epoch 45, best reward is 17\n",
      "Epoch 46, best reward is 20\n",
      "Epoch 47, best reward is 18\n",
      "Epoch 48, best reward is 19\n",
      "Epoch 49, best reward is 17\n",
      "Epoch 50, best reward is 13\n",
      "Epoch 51, best reward is 15\n",
      "Epoch 52, best reward is 14\n",
      "Epoch 53, best reward is 14\n",
      "Epoch 54, best reward is 14\n",
      "Epoch 55, best reward is 14\n",
      "Epoch 56, best reward is 10\n",
      "Epoch 57, best reward is 13\n",
      "Epoch 58, best reward is 13\n",
      "Epoch 59, best reward is 12\n",
      "Epoch 60, best reward is 13\n",
      "Epoch 61, best reward is 16\n",
      "Epoch 62, best reward is 14\n",
      "Epoch 63, best reward is 20\n",
      "Epoch 64, best reward is 18\n",
      "Epoch 65, best reward is 20\n",
      "Epoch 66, best reward is 10\n",
      "Epoch 67, best reward is 24\n",
      "Epoch 68, best reward is 14\n",
      "Epoch 69, best reward is 20\n",
      "Epoch 70, best reward is 25\n",
      "Epoch 71, best reward is 19\n",
      "Epoch 72, best reward is 19\n",
      "Epoch 73, best reward is 16\n",
      "Epoch 74, best reward is 18\n",
      "Epoch 75, best reward is 20\n",
      "Epoch 76, best reward is 28\n",
      "Epoch 77, best reward is 15\n",
      "Epoch 78, best reward is 19\n",
      "Epoch 79, best reward is 22\n",
      "Epoch 80, best reward is 18\n",
      "Epoch 81, best reward is 26\n",
      "Epoch 82, best reward is 21\n",
      "Epoch 83, best reward is 21\n",
      "Epoch 84, best reward is 14\n",
      "Epoch 85, best reward is 15\n",
      "Epoch 86, best reward is 21\n",
      "Epoch 87, best reward is 21\n",
      "Epoch 88, best reward is 18\n",
      "Epoch 89, best reward is 19\n",
      "Epoch 90, best reward is 20\n",
      "Epoch 91, best reward is 18\n",
      "Epoch 92, best reward is 24\n",
      "Epoch 93, best reward is 23\n",
      "Epoch 94, best reward is 27\n",
      "Epoch 95, best reward is 18\n",
      "Epoch 96, best reward is 20\n",
      "Epoch 97, best reward is 25\n",
      "Epoch 98, best reward is 23\n",
      "Epoch 99, best reward is 21\n",
      "Epoch 100, best reward is 21\n"
     ]
    }
   ],
   "source": [
    "# Game\n",
    "game_creation_func = lambda context: Tris()\n",
    "\n",
    "# Model\n",
    "layer_sizes = [27, 9, 3, 1]\n",
    "get_model_func = lambda: FeedForwardNetwork(layer_sizes, ActivationFunctions.get_sigmoid(), ActivationFunctions.get_sigmoid(), with_bias=False)\n",
    "\n",
    "# Game-Model interface\n",
    "def predict_func(model, game):\n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    available_actions = game.get_available_actions()\n",
    "    vs = []\n",
    "    for a in available_actions:\n",
    "        next_state = game.get_next_state(a)\n",
    "        flattened_board = next_state.flatten()\n",
    "        flattened_board_expanded = [[1,0,0] if cell == 1 else [0,1,0] if cell == 0 else [0,0,1] for cell in flattened_board]\n",
    "        flattened_board_expanded = np.array([cell for expanded_cell in flattened_board_expanded for cell in expanded_cell])\n",
    "        vs.append(model.predict(flattened_board_expanded)[0])\n",
    "    board_softmax = softmax(np.array(vs))\n",
    "    i_max = np.random.choice(list(range(len(board_softmax))), p=[p for l in board_softmax for p in l])\n",
    "    best_action = available_actions[i_max]\n",
    "    return best_action\n",
    "agent_wrapper_func = get_agent_wrapper_func(predict_func)\n",
    "\n",
    "# Reward function and update env function\n",
    "# Since Tris is a two-players game, we can compute the reward with a tournament against the current generation\n",
    "keep_only = 100\n",
    "reward_func, get_context_func = RewardBuilderGame() \\\n",
    ".with_game_creation_func(game_creation_func) \\\n",
    ".with_agent_wrapper_func(agent_wrapper_func) \\\n",
    ".with_competitive_tournament(TournamentMode.VS_BESTS_IF_INCREASE) \\\n",
    ".with_keep_only(keep_only) \\\n",
    ".get()\n",
    "\n",
    "# Update population function\n",
    "get_new_pop_func = PopulationUpdateBuilderStatic() \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.001, 1.) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.003, 1.) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.006, 1.) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.01, 1.) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.001, 0.5) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.003, 0.5) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.06, 0.5) \\\n",
    ".add_operator(\"es_n_rewards_gradient\", 0.1, 0.01, 0.5) \\\n",
    ".add_selector_func(SelectorFunctionFactory.get_geometric_selector_function(0.3)) \\\n",
    ".get() # learning rate, sigma, keep perc \n",
    "#.add_operator(\"es_1_copy\", 0.1) \\\n",
    "\n",
    "# Evolution function\n",
    "evolution_func = EvolutionBuilder() \\\n",
    "    .with_get_model_func(get_model_func) \\\n",
    "    .with_reward_func(reward_func) \\\n",
    "    .with_get_new_pop_func(get_new_pop_func) \\\n",
    "    .with_get_context_func(get_context_func) \\\n",
    "    .get()\n",
    "\n",
    "pop_size = 100\n",
    "num_epochs = 100\n",
    "experiment_name = \"tris_gradient_and_mutation\"\n",
    "clean_experiment_directory(experiment_name)\n",
    "evolution_func(experiment_name, pop_size, num_epochs, num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
